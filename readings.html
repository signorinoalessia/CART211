<!doctype html>
<html>
<head>
	<meta charset="UTF-8">
	<title>CART211: A.Signorino</title>
	<link rel="stylesheet" href="css/style.css">
</head>

<body>
	<section id="s1">
		<header>
			<h1>CART 211: Alessia Signorino</h1>
			<h2>Readings</h2>
		</header>

		<nav>
			<p2><a href="index.html">Home</a></p2>
			<p2><a href="exercises.html">Exercises</a></p2>
			<p2><a href="readings.html">Readings</a></p2>
			<p2><a href="projects.html">Projects</a></p2>
		</nav>

		</section>

	<article>
		<h2>Vannvar Bush, As We May Think</h2>
		<p class="reading">
			In the essay published by Vannevar Bush in 1945, there are many chilling points raised that would lead the reader to believe that he traveled in time to catch a glimpse of the future. He recalls scientists about how they joined forces under the obligation of war conditions for physicists, biologists and scientists towards the common cause of weaponry and destructive technology. Once war was over, he suggests to treasure the same environment of sharing information, due to the necessity of bridging disciplines for innovation. Nonetheless, there was the challenge of the increasing quantity of research accumulating and not being stored efficiently, thus causing the intended researchers of the field to be unable to unearth information. There is an interesting duality of such a challenge fabricated by scientific persons: information was becoming more specialized and in a growing quantity. The ways of information publication has been extended beyond the current means, especially since at the time, archiving systems were still primarily analogue. Vannevar recognizes the arrival
			of mechanical aids, but that inventions depend greatly on the economic landscapes they are set in. For example, two centuries prior, Leibnitz’s invention was not developed because in its current context, the labour and cost outweighed the positive results of the labour saved. Without the proper finances, it is difficult to justify inverting energy and labour into them. In the 40s, technology was becoming gradually a lot more affordable, thus allowing for expansion of existing prototypes.
			The existing concept of recording information relied on different physical mediums, such as wax, film and magnetics wires.  Camera technology was becoming smaller and could accommodate finer grained-sensitive compounds. Despite further developments, photography always required wetting the plate or the film during development. Vannevar wishes how technology could requires less steps to get to tangible results. “Often it would be advantageous to be able to snap the camera and to look at the picture immediately”. This quote is so wonderful because it points out an aspect that is now so ubiquitous that it is taken for granted. The author also explores the potential of television equipment that uses a beam of electrons instead of a moving pointer, and a screen which glows when the electrons hit it instead of chemically modifying matter. Vannevar ponders upon the possible development of this technology and foresaw the concept of hard drives, the idea of storing great amounts of information into a disproportionate smaller receptacle. He expands on auditory reproductions, such as the first voice synthetiser. Voder was proof of a machine talking back without the vocal input of a human. The Vocoder would pick up sound.The Vocoder was similar to the stenotype, in the manner which it was only understood but those that were trained into using it. The mechanization of languages was clunky especially in the scientific field. Vannevar dreamt also of remote storage of data, where an investigator needs not to be in a static place to record, type, photograph. He compares the punched-card machines that record info positionally in which operations are delegated to other machines. But most importantly, he raises the thought about the interface of access being as vital as the dissemination of information. Vannevar’s musings touch incredibly close to the various technologies that one uses everyday. His essay suggests the birth of computers and the Internet, almost as if he was foreseeing that humanity was at the edge of a revolution of information systems.
		</p>

		<h2>Roy Rosenzweig, Writing the history of the Internet</h2>
		<p class="reading">
				In the article Writing the history of the Internet,  Roy Rosenzweig asks the reader to think of any reference of the Internet in Post-World War II American history textbooks, implying that there is not much written on the matter. The author proceeds to speculate on how history will be written and to keep the biographic, bureaucratic, ideological and social aspects in mind. The internet was far from being a novel invention, even in the 90s when this article was written. It was difficult to depict where the future of the Internet was headed, but Roy was certain that it would be a strong presence in different spheres of our society and that technology would de-centralize from the environment of scientists and bureaucrats that had helped build it. Roy saw the potential of the Internet to support commercial and leisure activities such as watching a movie, shopping and socializing. The context in which the Arpanet was born is a crucial factor to the open and accessible philosophy that is quintessential to the Internet. The 60s in America were times in which the youth was undergoing a countercultural revolution which grouped university students rejecting proprietary computer systems as well as more conventional hippies. The author writes that the vision of an open Internet came from hackers, such as Bill Gates and Paul Allen during their teenager years spreading programs for free over the Internet. I feel a bit skeptical about giving merit to a wave of values to a handful of individuals. Rather, I believe American society’s cultural changes along with the beginning of commercialisation of a new technology must have created an environment where anything seemed possible. Thinkers were creating manifestos such as Computer Lib and called people to arms against bureaucracies. Prior to the 1980s, the Internet was a relatively open area in which anyone with a computer and access to Internet could have a voice in. But by 1980s, many services, hardware and softwares were getting privatized and agglomerated by growing tech giants. In order to decentralize the Internet, it was necessary to demonopolize the control that Microsoft and Intel had on personal computers. At some point, Bill Gates’ company was close to own all the browsers from which to access Internet, which went against the very core value of what the Internet is. The Internet has been shaped not only by its creators, but its users, the Netizens, but from hackers, advocates of a free space as well.
		</p>

		<h2>Rachel Greene, "Web Work: A History of Internet Art</h2>
		<p class="reading">
			The term net.art was coined from an accident resulting from a software glitch that happened to artist Vuk Cosic when he opened an anonymous email. The height of this movement was in the years between 1994 and 1998, where artists laid out their initial ambitions of what net.art could be as well as setting up collective ideas and goals. These net.artists communicated via email, the dominant mode of communication. This method allowed net.artists to communicate directly without passing through institutions or bureaucracy. This movement had a penchant for publishing manifestos and voicing their political views through online publications. The community in Europe was reacting to changes occuring in Europe during the 90s. Within the East-European context, the government and the academics were shifting towards a “civil society” that was post-Communist and neo-liberal, and the Internet seemed utopic and instrumental in accompanying the welcomed change. At the time, the Internet of the era was uncluttered, populated mostly by personal pages, community online communities and technology companies. Additionally, bulletin board systems were not populated by people with ties to art institutions. Sites such as jodi.org acquired a cult status in the new-media art world, triggering collaborations with American companies. By 1996, the Internet was becoming more significant in the art and cultural landscapes, however online publications that hosted niche content made little profit. Net.artists needed to explore various ways of sustaining their movement financially while bracing for the colonization of corporations and mainstream media which threatened to change the Internet that it was. The community of net.artists involved a considerable amount of women such as the VNS collective who published their ‘Cyberfeminist Manifesto” in 1991. Notable net.artist works include  “My Boyfriend Came Back From The War” of Olia Lialina that explores a romantic relationship, PTSD and alternative storytelling in a melancholic environment. The user navigates the space by dragging multiple frames of the browser window in a text-based and image dialogue that is akin to a stream of consciousness. Commissioned by the Guggenheim, Shu Lea Cheang created their first website. An ode to Brandon’s story, the life of a trans man that was portrayed in Boys Don’t Cry, plays with the notion of gender. There were also instances of identity theft, such as an unidentified user publishing text to Rhizome and Nettime unde the name of existing critics, or a user with the pseudonym Keiko Suzuki borrowing the name 7-11 to create a list-serve. 7-11 became a platform onto which users posted their junk, email art, and jokes. An epic instance of tension between net.artists and institutions happened after documenta decided that they would take the website offline to sell as a CD-ROM which prompted Slovenian artist Vuk Cosic, the creator of the name of the art movement, to copy the site and post it on his server. The website would be named Documenta Done. Cosic circulated news of the event as a “major international art theft” and called himself an “Eastern European Hacker”. This clever instance highlights that uncomfort of adapting to a new institutional process.
		</p>

		<h2>Richard Stallman, The GNU Manifesto</h2>
		<p class="reading">
			The GNU manifesto was written by Richard Stallman in 1985 and was part of the Free Software Movement, a campaign that demanded freedom for users of software. However, GNU is not open-source as the latter was coined from user that diverged from the Free Software movement due to their ethical values. GNU stands for GNU is not Unix, which was an operating system that was free to everyone who can use it. GNU’s golden rule encourages that if one likes the program that they have made, that they must share it with people who like it. As a former employee of the Artificial Intelligence Lab, Stallman declares that software sellers want to divide the users by making each user agree not to share with others. He believes that to comply to terms of non-disclosure means breaking solidarity with other users. So in order to use computers “without dishonour”, he put together a core group of softwares to get by without the use of paid softwares . The distinction with public domain means that although anyone is free to modify and redistribute GNU, no one is allowed to place proprietary modifications, or to restrict the redistribution in any way. GNU’s philosophy is to treat other fellow programmers as friends and to see programming beyond a way of making money. The notion of creating softwares that allows programmers to obey the law while sharing their work was a great motivator for Stallman.
			Just like musicians will still play music regardless if they are paid or not, programmers will do so for the pleasure of activity. Therefore eager programmers can contribute to the cause by donating programs and work and computer manufacturers can donate machines and benefit from having the option of a free system software and use the project as an educational tool. A free tool is profitable for companies and any available programmer can be offered payment to offer support without relying on a specific software vendor, as clients already buy products accompanied by a service. Social contribution is not as financially lucrative, however Stillman argues that programmers can still manage to make a living through different methods, such as selling teaching, hand holding and maintenance services.He encourages programmers to look at the borade picture of the possible benefits that freeware creates.
		</p>

		<h2>Brian Christian, Mind vs. Machine</h2>
		<p class="reading">
			Turing Test experience of reading what distinguishes human minds from machines. The Seattle author portrays his experience participating at the Turing test, a test that takes place every year for the past two decades, in which artificial-intelligence scientists congregate for the field’s most anticipated competition to win the Loebner Prize. The British mathematician Alan Turing and pioneer of computer science challenged humans to build a computer so sophisticated that it could appear to have a mind.The author took the role of the human confederate which would be put against a computer and convince in the span of a five minute conversation that they were the real human. So far, only a computer program came close to winning in 2008, by missing a mark by a single vote. The author jokingly described his role as a defender of the human race. The humans’s general instruction is to be themselves, for they would already know instinctively how to be human. But to the author’s recollection of previous transcripts, to be simply oneself is too banal and resulted in uninspiring exchanges with the judges. The meaning and place of computers in human society, such as was the case from the mid-18th century in which mostly women performed calculations and numerical analysis that allowed scientific exploits such as the predictions of Halley’s Comet and the Manhattan Project at Los Alamos. Computer was the term used to refer to the human compilers of information and the default term pointed to the human computer. In the 21st century, it is the digital computer that has become the default. Many theorists of different practices have pondered on the elements that define the human being. What are the features that only humans possess as opposed to other animals? Humans used to be the only one capable of using language and mathematics, but that is hardly the case in our current reality. Should the definition of the human be reactive to an advancing form of technology? Research in AI highlights what intelligence and selfhood is not defined by.The Turing Test is devoted to challenging the abilities of communication rather than the entirety of intelligence with a capital “i”. Language communicates information, as well as emotional intelligence, the latter being hard to imitate. Humans follow conventions that are dependant on socio-cultural factors that have to be taken in consideration.
		</p>

		<h2>Jeremy Reimer, The History of the GUI</h2>
		<p class="reading">
			GUI stands for graphical user interface and has been adopted as primary mode of interaction of computers. Reimer’s text explores the reasons of this technical evolution. The idea of GUI began with Vannevar Bush’s idea of the Memex device in the 1930s, which consisted of a desk with two touch screen graphical displays, a keyboard and a scanner. His concept was not widely discussed at the time. Around the end of the 30s, digital computers beginning to get built to have an edge against the enemy. It was at this period that Vannevar Bush published his famous As We May Think essay in the Atlantic Monthly, which propelled Douglas Engelbart, creator of the GUI, to build this machine in 1968. As an electrical engineer, Douglas had an epiphany at work in regards of his purpose in life. He realized that he wanted to benefit all humanity not just a few individuals. He sought funding and by 1959 he was supported by the United States Air Force. He envisioned the computer as a tool to enhance human intellect, by providing a clearer solution to approach a problem situation. His ideas were radical. For example, he thinks of a program where architects using something similar to modern CAD software, and even a real-time text-based terminal. Douglas set-up a demonstrated where he attempted to track his interaction, which was instrumental given that his approach was quite new for the times. Television cameras were pointed on his face, his hands and small display screen. The display consisted of uppercase letters and lines on the screen, a standard typewriter style keyboard, a five-key chording keyboard and a rudimentary mouse, a small box with three buttons. The mouse was found to be the most natural way of controlling an on-screen cursor as opposed to touch screens and light screens. This demo also featured concept such as hypertext linking, full-screen document editing, networked document collaboration, e-mail and video conferencing. Unfortunately, the funding was insufficient and by 1989 Douglas’s institution had to close down. Xerox, for fear of being left behind in a paperless future undertook the goal of controlling this newfound technology. The Palo Alto Research Center (PARC) attracted top computer scientists in the country for the freedom that they were allowed in their research. Xerox implemented the Xerox Alto computer which introduced the modern pointer. The programming language Smalltalk was designed so that it was easily understandable and was additionally a graphical development environment. The combination of Xerox Alto and Smalltalk foreshadow what the modern personal computer would become. Many engineers had migrated to Apple and would develop the Lisa, a graphical computer whose interface design was still being defined and who introduced new GUI concepts such as double-clicking, checking selected files and drag and drop. The Lisa was too expensive and a cheaper counterpart with a smaller memory was later designed. Competition inspired Bill Gates to create Windows 1.01 who embraced window control widgets, menus and scrollbars for each individual window placed below the title bar. Tandy DeskMate, GEM, Amiga Workbench, GEOS, Acorn, NeXTSTEP would later follow their steps.By the end of the 1980s, new GUIs that imitated the Windows appearance but using Unix would be created. With the rise of Unix, Linux and FreeBSD more open-source desktop environments were in demand. With the 90s, Windows was the most popular despite lacking features that Macintosh had. GUIs have radically changed the way that we interact with machines, despite the core functionality of GUIs remaining the same, they still have the potential of offering new way of interacting.
		</p>

		<h2>Sherry Turkle, "Video Games and Computer Holding Power"</h2>
		<p class="reading">
			Video games are a major force in popular culture since the early 1980s. They become a telling demonstrator of how people from different spectrum of ages interact with computers. Approaching computing from the field of psychoanalysis, Turkle explores how the computer allows people to use personae to interact in a different way they would not do in a non-computing situation. The Internet becomes a social laboratory for experimenting with the construction of the self. Turkle questions whether this form of expressing identity is a shallow game of imitation or rather an emerging manner of thinking about the mind. By 1983, computers had become an active part of daily life in addition to the more significant presence of the computer. Children proved to be instinctively at ease with this influx of technology, which accentuated the age gap due to the differences in expertise. Video Games, unlike television, is an active activity, and in certain way, something you become.These games demand complex and diverse skills in which one assimilates large amounts of information and learning how to strategize. Turkle mentions the strategies involved in Pac-Man, such as shifting from attack and defense, and seeking the convenient timing to accumulate points, all while thinking fast at the risk of being eaten. Element of competitive community and recognition were instigated by the simple gesture of adding initials on a screen to track highest score. Additionally, video games unlike pinball, increase elements in difficulty. This amnesia of analog games such as pinball cannot compete with the potential of memory found in video games. For example, Jarish, an avid 12-years old game player, started modifying games to make them more complicated. This desire became the gateway for his interest in computers. Unfortunately for him and other fellow programmers and hobbyists, many games were protected to prevent people from copying, which proves to be frustrating to Jarish because he enjoys the act of modifying code. To players like Jarish, video games show what is possible to do with computers. Despite the limitations of computer graphics to have more realistic gestures and facial expressions, video games create microworlds which allow one to pursue possible directions of story. This medium becomes a way of telling stories beyond traditional film images. Games have overlapping elements with that of making a film in the way that stories can be told. Games also allow certain players of feeling in control of a situation and where the user is encouraged to use their concentration in a meditative way. Games offer a way to recenter, to fantasize yourself as someone else, explore and test oneself against the elements. Playing in a controlled environment creates a situation in which the user can hardly blame outside conditions for their failures. The qualities that are sought from a game vary depending on one’s taste. For example, David, a lawyer, prefers games that reflect back the skills of the players in a measurable way. Games for him become a reassurance and a place to test himself.
		</p>

	</article>

	<footer> No pixels were harmed in the making of this site. 2018 &copy;</footer>

</body>
</html>
